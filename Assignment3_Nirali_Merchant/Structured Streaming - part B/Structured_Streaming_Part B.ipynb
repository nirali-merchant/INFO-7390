{"cells":[{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# To make sure that this notebook is being run on a Spark 2.0 cluster, let's see if we can access the SparkSession - the new entry point of Apache Spark 2.0.\n# If this fails, then you are not connected to a Spark 2.0 cluster. Please recreate your cluster and select the version to be \"Spark 2.0 (Scala 2.10)\".\nspark"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%fs ls /FileStore/tables"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Data"],"metadata":{}},{"cell_type":"code","source":["%fs head /FileStore/tables/Online_Retail_formatted_1-3daf0.csv"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Batch/Interactive Processing"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ninputPath = \"/FileStore/tables/\"\n\n# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\n\n\n# Static DataFrame representing data in the CSV files\nstaticInputDF = (\n  spark\n    .read\n    .format(\"csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferschema\",\"true\")\n    .load(inputPath+\"/*.csv\")\n  \n)\n\ndisplay(staticInputDF)\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["staticInputDF.count()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndf = staticInputDF.withColumn(\"InvoiceDateTime\", unix_timestamp(\"InvoiceDate\", \"MM/dd/yyyy HH:mm\")\n    .cast(\"timestamp\"))\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.sql.functions import *      # for window() function\n\nstaticCountsDF = (\n  df\n  .groupBy(window(df.InvoiceDateTime, \"1 week\") , df.Country)\n  .count()\n)\nstaticCountsDF.cache()\n\n# Register the DataFrame as table 'static_counts'\nstaticCountsDF.createOrReplaceTempView(\"static_counts\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql select * from static_counts"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Transactions processed by each country in one week"],"metadata":{}},{"cell_type":"code","source":["%sql select Country, date_format(window.start, \"MM-dd-yy\") as date, count as Transactions from static_counts order by  date,Country "],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ncsvschema = StructType([ StructField(\"InvoiceNo\", StringType(), True), StructField(\"StockCode\", StringType(), True),StructField(\"Description\", StringType(), True),StructField(\"Quantity\", IntegerType(), True),StructField(\"UnitPrice\", DoubleType(), True),StructField(\"Sales\", DoubleType(), True),StructField(\"InvoiceDate\", StringType(), True),StructField(\"CustomerID\", IntegerType(), True) ,StructField(\"Country\", StringType(), True),StructField(\"InvoiceDateTime\", TimestampType(), True)])\n# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream\n    .format(\"csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferschema\",\"true\")\n    .option(\"maxFilesPerTrigger\", 1)\n    .schema(csvschema)\n    .load(inputPath+\"/*.csv\")\n    \n    \n)\n\n\nstreamingInputDF = streamingInputDF.withColumn(\"InvoiceDateTime\", unix_timestamp(\"InvoiceDate\", \"MM/dd/yyyy HH:mm\")\n    .cast(\"timestamp\"))\n\n\n\n\n# Same query as staticInputDF\n\n\n\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  streamingInputDF\n  .groupBy(window(streamingInputDF.InvoiceDateTime, \"1 week\") , streamingInputDF.Country, streamingInputDF.Description)\n  .count()\n)\n\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["display(streamingInputDF)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["from time import sleep\nsleep(15)  # wait a bit for computation to start"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%sql select count(*) from counts"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Transactions processed by each country in 1 week"],"metadata":{}},{"cell_type":"code","source":["%sql select Description, Country, date_format(window.start, \"MM-dd-yy\") as date, count as Transactions from counts order by  date,Country,Description"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["sleep(5)  # wait a bit for computation to start"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%sql select Description, Country, date_format(window.start, \"MM-dd-yy\") as date, count as Transactions from counts order by  date,Country,Description"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%sql select count(*) from counts"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["sleep(5)  # wait a bit for computation to start"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%sql select Description, Country, date_format(window.start, \"MM-dd-yy\") as date, count as Transactions from counts order by  date,Country,Description"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["sleep(5)  # wait a bit for computation to start"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["sleep(10)  # wait a bit more for more data to be computed"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["%sql select Description, Country, date_format(window.start, \"MM-dd-yy\") as date, count as Transactions from counts order by  date,Country,Description"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["%sql select count(*) from counts"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":33}],"metadata":{"name":"Structured_Streaming_Part B","notebookId":3549806646550895},"nbformat":4,"nbformat_minor":0}
